The Mean of the Review Attribute is : 
4.12043007805
Done
55558
[('not', 80912), ('baby', 70749), ('one', 66194), ('love', 52997), ('great', 47666), ('like', 45664), ('would', 45661), ('use', 42480), ('seat', 39416), ('get', 38306), ('month', 34560), ('time', 33391), ('little', 33166), ('easy', 32862), ('old', 31945), ('well', 30745), ('product', 30585), ('really', 28026), ('also', 27756), ('son', 26691), ('bought', 25451), ('work', 25281), ('no', 24775), ('good', 23749), ('much', 23651)]
Iteration 1, loss = 0.45428524
Iteration 2, loss = 0.40785258
Iteration 3, loss = 0.40400331
Iteration 4, loss = 0.39988986
Iteration 5, loss = 0.39540057
Iteration 6, loss = 0.39046301
Iteration 7, loss = 0.38504388
Iteration 8, loss = 0.37914288
Iteration 9, loss = 0.37282187
Iteration 10, loss = 0.36621321
Iteration 11, loss = 0.35945509
Iteration 12, loss = 0.35270799
Iteration 13, loss = 0.34612846
Iteration 14, loss = 0.33976383
Iteration 15, loss = 0.33370657
Iteration 16, loss = 0.32796480
Iteration 17, loss = 0.32254851
Iteration 18, loss = 0.31745792
Iteration 19, loss = 0.31264645
Iteration 20, loss = 0.30810095
Iteration 21, loss = 0.30379660
Iteration 22, loss = 0.29972767
Iteration 23, loss = 0.29586998
Iteration 24, loss = 0.29218484
Iteration 25, loss = 0.28871540
Iteration 26, loss = 0.28540752
Iteration 27, loss = 0.28227060
Iteration 28, loss = 0.27929918
Iteration 29, loss = 0.27649317
Iteration 30, loss = 0.27383183
Iteration 31, loss = 0.27130552
Iteration 32, loss = 0.26893119
Iteration 33, loss = 0.26665389
Iteration 34, loss = 0.26452099
Iteration 35, loss = 0.26249006
Iteration 36, loss = 0.26055279
Iteration 37, loss = 0.25872327
Iteration 38, loss = 0.25698454
Iteration 39, loss = 0.25532555
Iteration 40, loss = 0.25373439
Iteration 41, loss = 0.25222038
Iteration 42, loss = 0.25079553
Iteration 43, loss = 0.24942148
Iteration 44, loss = 0.24811018
Iteration 45, loss = 0.24684358
Iteration 46, loss = 0.24562329
Iteration 47, loss = 0.24446216
Iteration 48, loss = 0.24334388
Iteration 49, loss = 0.24226096
Iteration 50, loss = 0.24122996
Iteration 51, loss = 0.24023739
Iteration 52, loss = 0.23925876
Iteration 53, loss = 0.23833274
Iteration 54, loss = 0.23742012
Iteration 55, loss = 0.23656763
Iteration 56, loss = 0.23572151
Iteration 57, loss = 0.23490310
Iteration 58, loss = 0.23411858
Iteration 59, loss = 0.23332265
Iteration 60, loss = 0.23259551
Iteration 61, loss = 0.23187653
Iteration 62, loss = 0.23116057
Iteration 63, loss = 0.23047349
Iteration 64, loss = 0.22984240
Iteration 65, loss = 0.22917355
Iteration 66, loss = 0.22855993
Iteration 67, loss = 0.22794612
Iteration 68, loss = 0.22735955
Iteration 69, loss = 0.22677985
Iteration 70, loss = 0.22618767
Iteration 71, loss = 0.22565817
Iteration 72, loss = 0.22511511
Iteration 73, loss = 0.22457688
Iteration 74, loss = 0.22408236
Iteration 75, loss = 0.22357121
Iteration 76, loss = 0.22313068
Iteration 77, loss = 0.22262388
Iteration 78, loss = 0.22216561
Iteration 79, loss = 0.22171743
Iteration 80, loss = 0.22125932
Iteration 81, loss = 0.22084553
Iteration 82, loss = 0.22040956
Iteration 83, loss = 0.21998303
Iteration 84, loss = 0.21958900
Iteration 85, loss = 0.21919452
Iteration 86, loss = 0.21879958
Iteration 87, loss = 0.21842603
Iteration 88, loss = 0.21803642
Iteration 89, loss = 0.21769104
Iteration 90, loss = 0.21732696
Iteration 91, loss = 0.21697898
Iteration 92, loss = 0.21660942
Iteration 93, loss = 0.21629107
Iteration 94, loss = 0.21594038
Iteration 95, loss = 0.21563227
Iteration 96, loss = 0.21530973
Iteration 97, loss = 0.21497220
Iteration 98, loss = 0.21467268
Iteration 99, loss = 0.21439562
Iteration 100, loss = 0.21410273
Iteration 101, loss = 0.21380711
Iteration 102, loss = 0.21352207
Iteration 103, loss = 0.21321185
Iteration 104, loss = 0.21293639
Iteration 105, loss = 0.21267558
Iteration 106, loss = 0.21241400
Iteration 107, loss = 0.21213511
Iteration 108, loss = 0.21184569
Iteration 109, loss = 0.21160939
Iteration 110, loss = 0.21136867
Iteration 111, loss = 0.21112632
Iteration 112, loss = 0.21087022
Iteration 113, loss = 0.21064084
Iteration 114, loss = 0.21039623
Iteration 115, loss = 0.21014292
Iteration 116, loss = 0.20992464
Iteration 117, loss = 0.20967873
Iteration 118, loss = 0.20947432
Iteration 119, loss = 0.20926199
Iteration 120, loss = 0.20905398
Iteration 121, loss = 0.20881901
Iteration 122, loss = 0.20860994
Iteration 123, loss = 0.20840057
Iteration 124, loss = 0.20819419
Iteration 125, loss = 0.20799900
Iteration 126, loss = 0.20778314
Iteration 127, loss = 0.20757597
Iteration 128, loss = 0.20740428
Iteration 129, loss = 0.20721864
Iteration 130, loss = 0.20700372
Iteration 131, loss = 0.20682309
Iteration 132, loss = 0.20666845
Iteration 133, loss = 0.20645396
Iteration 134, loss = 0.20625922
Iteration 135, loss = 0.20612441
Iteration 136, loss = 0.20592696
Iteration 137, loss = 0.20574553
Iteration 138, loss = 0.20556847
Iteration 139, loss = 0.20541479
Iteration 140, loss = 0.20522503
Iteration 141, loss = 0.20504349
Iteration 142, loss = 0.20491218
Iteration 143, loss = 0.20471574
Iteration 144, loss = 0.20455789
Iteration 145, loss = 0.20441174
Iteration 146, loss = 0.20423770
Iteration 147, loss = 0.20407887
Iteration 148, loss = 0.20391687
Iteration 149, loss = 0.20379300
Iteration 150, loss = 0.20364465
Iteration 151, loss = 0.20346746
Iteration 152, loss = 0.20335890
Iteration 153, loss = 0.20318519
Iteration 154, loss = 0.20304695
Iteration 155, loss = 0.20289343
Iteration 156, loss = 0.20274420
Iteration 157, loss = 0.20261341
Iteration 158, loss = 0.20241961
Iteration 159, loss = 0.20233078
Iteration 160, loss = 0.20215346
Iteration 161, loss = 0.20204559
Iteration 162, loss = 0.20189541
Iteration 163, loss = 0.20175730
Iteration 164, loss = 0.20165624
Iteration 165, loss = 0.20148854
Iteration 166, loss = 0.20137749
Iteration 167, loss = 0.20125241
Iteration 168, loss = 0.20111163
Iteration 169, loss = 0.20097829
Iteration 170, loss = 0.20087308
Iteration 171, loss = 0.20072504
Iteration 172, loss = 0.20062010
Iteration 173, loss = 0.20050904
Iteration 174, loss = 0.20034972
Iteration 175, loss = 0.20027140
Iteration 176, loss = 0.20013513
Iteration 177, loss = 0.20001841
Iteration 178, loss = 0.19991923
Iteration 179, loss = 0.19979092
Iteration 180, loss = 0.19967731
Iteration 181, loss = 0.19955015
Iteration 182, loss = 0.19942555
Iteration 183, loss = 0.19931340
Iteration 184, loss = 0.19921865
Iteration 185, loss = 0.19908264
Iteration 186, loss = 0.19897241
Iteration 187, loss = 0.19889987
Iteration 188, loss = 0.19876853
Iteration 189, loss = 0.19863327
Iteration 190, loss = 0.19854781
Iteration 191, loss = 0.19845249
Iteration 192, loss = 0.19833496
Iteration 193, loss = 0.19821732
Iteration 194, loss = 0.19811667
Iteration 195, loss = 0.19802332
Iteration 196, loss = 0.19790296
Iteration 197, loss = 0.19781473
Iteration 198, loss = 0.19771648
Iteration 199, loss = 0.19763028
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Accuracy on the Training Dataset
91.8171414474
Done
27828
[('not', 20502), ('baby', 17687), ('one', 16201), ('love', 13132), ('great', 11756), ('would', 11417), ('like', 11267), ('seat', 10442), ('use', 10437), ('get', 9549), ('month', 8510), ('little', 8383), ('time', 8267), ('easy', 8255), ('old', 7899), ('well', 7800), ('product', 7426), ('really', 6923), ('also', 6870), ('son', 6468), ('work', 6259), ('bought', 6186), ('no', 6051), ('good', 5950), ('much', 5944)]
Accuracy on the Testing Dataset
73.8925309268
             precision    recall  f1-score   support

          0       0.30      0.08      0.12      8668
          1       0.77      0.95      0.85     27789

avg / total       0.66      0.74      0.67     36457

